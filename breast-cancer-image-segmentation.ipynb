{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:30:54.975802Z","iopub.execute_input":"2023-06-10T00:30:54.976396Z","iopub.status.idle":"2023-06-10T00:30:54.983680Z","shell.execute_reply.started":"2023-06-10T00:30:54.976356Z","shell.execute_reply":"2023-06-10T00:30:54.982275Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom PIL import Image\nimport cv2\n\nimport tensorflow as tf\nfrom tf.keras.utils import load_img\nfrom keras.models import Model\nfrom keras.layers import Conv2d, layer, Dense, Input\n\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:30:54.985616Z","iopub.execute_input":"2023-06-10T00:30:54.986172Z","iopub.status.idle":"2023-06-10T00:30:57.913049Z","shell.execute_reply.started":"2023-06-10T00:30:54.986140Z","shell.execute_reply":"2023-06-10T00:30:57.911348Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_img\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2d, layer, Dense, Input\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'"],"ename":"ModuleNotFoundError","evalue":"No module named 'tf'","output_type":"error"}]},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:31:23.590625Z","iopub.execute_input":"2023-06-10T00:31:23.591038Z","iopub.status.idle":"2023-06-10T00:31:23.597632Z","shell.execute_reply.started":"2023-06-10T00:31:23.591003Z","shell.execute_reply":"2023-06-10T00:31:23.596177Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"benign_path = \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign\"\nnormal_path = \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/normal\"\nmalignant_path = \"/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/malignant\"\nbenign_dataset = []\nnormal_dataset = []\nmalignant_dataset = []\n\nclasses_path = [benign_path, normal_path, malignant_path]\nclasses_dataset = [benign_dataset, normal_dataset, malignant_dataset]\n\nfor index, _class in enumerate(classes_path):\n    for filename in os.listdir(_class):\n        image_path = os.path.join(_class, filename)\n        \n        if os.path.isfile(image_path) and filename.lower().endswith(').png'):\n            mask_path = filename.replace('.png', '_mask.png')\n            filename = os.path.join(_class, filename) \n            mask_path = os.path.join(_class, mask_path) \n            classes_dataset[index].append((filename ,mask_path))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:31:27.098788Z","iopub.execute_input":"2023-06-10T00:31:27.099150Z","iopub.status.idle":"2023-06-10T00:31:27.658404Z","shell.execute_reply.started":"2023-06-10T00:31:27.099118Z","shell.execute_reply":"2023-06-10T00:31:27.657283Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"color_yellow = '\\033[33m' \ncolor_black = '\\033[30m'\n\nlength = 0\nfor _class in classes_dataset:\n    print('The length of the {} class is {}'.format(_class[0][0].split('/')[-2], len(_class)))\n    length += len(_class)\nprint(color_yellow + 40*'-')\nprint(color_black + 'The total length of the dataset is ' + str(length))","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:31:31.735952Z","iopub.execute_input":"2023-06-10T00:31:31.737145Z","iopub.status.idle":"2023-06-10T00:31:31.746741Z","shell.execute_reply.started":"2023-06-10T00:31:31.737106Z","shell.execute_reply":"2023-06-10T00:31:31.745399Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The length of the benign class is 437\nThe length of the normal class is 133\nThe length of the malignant class is 210\n\u001b[33m----------------------------------------\n\u001b[30mThe total length of the dataset is 780\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass AttentionEncoder(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(AttentionEncoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.linear = nn.Linear(input_size, hidden_size)\n        self.attention = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        # x: input image tensor of shape (batch_size, input_size)\n        hidden = torch.tanh(self.linear(x))\n        attention_weights = F.softmax(self.attention(hidden), dim=1)\n        context_vector = torch.sum(hidden * attention_weights, dim=1)\n        return context_vector\n\n\nclass AttentionDecoder(nn.Module):\n    def __init__(self, hidden_size, num_classes):\n        super(AttentionDecoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.linear = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, context_vector):\n        # context_vector: input context vector tensor of shape (batch_size, hidden_size)\n        output = self.linear(context_vector)\n        return output\n\n\nclass AttentionClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(AttentionClassifier, self).__init__()\n        self.encoder = AttentionEncoder(input_size, hidden_size)\n        self.decoder = AttentionDecoder(hidden_size, num_classes)\n\n    def forward(self, x):\n        # x: input image tensor of shape (batch_size, input_size)\n        context_vector = self.encoder(x)\n        output = self.decoder(context_vector)\n        return output\n\nclass ImageEncoderDecoder(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(ImageEncoderDecoder, self).__init__()\n        self.encoder = AttentionEncoder(input_size, hidden_size)\n        self.decoder = AttentionDecoder(hidden_size, num_classes)\n\n    def forward(self, x):\n        context_vector = self.encoder(x)\n        output = self.decoder(context_vector)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:48:23.905938Z","iopub.execute_input":"2023-06-10T00:48:23.906335Z","iopub.status.idle":"2023-06-10T00:48:23.916767Z","shell.execute_reply.started":"2023-06-10T00:48:23.906302Z","shell.execute_reply":"2023-06-10T00:48:23.915836Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Assuming you have prepared your dataset and defined the model as 'model'\nnum_classes = 10\n\nmodel = ImageEncoderDecoder(input_size, hidden_size, num_classes)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    \n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * images.size(0)\n    \n    train_loss /= len(train_loader.dataset)\n    \n    # Evaluation\n    model.eval()\n    valid_loss = 0.0\n    correct = 0\n    \n    with torch.no_grad():\n        for images, labels in valid_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            valid_loss += loss.item() * images.size(0)\n            \n            _, predicted = torch.max(outputs.data, 1)\n            correct += (predicted == labels).sum().item()\n    \n    valid_loss /= len(valid_loader.dataset)\n    accuracy = correct / len(valid_loader.dataset)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Save the best model checkpoint\ntorch.save(model.state_dict(), 'best_model.pt')\n\n# Load the best model checkpoint for testing\nmodel.load_state_dict(torch.load('best_model.pt'))\n\n# Testing\nmodel.eval()\ntest_loss = 0.0\ncorrect = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item() * images.size(0)\n        \n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == labels).sum().item()\n\ntest_loss /= len(test_loader.dataset)\naccuracy = correct / len(test_loader.dataset)\n\nprint(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T00:48:25.364862Z","iopub.execute_input":"2023-06-10T00:48:25.365816Z","iopub.status.idle":"2023-06-10T00:48:25.447997Z","shell.execute_reply.started":"2023-06-10T00:48:25.365751Z","shell.execute_reply":"2023-06-10T00:48:25.446671Z"},"trusted":true},"execution_count":44,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     18\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[1;32m     21\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"],"ename":"NameError","evalue":"name 'train_loader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}