{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T00:30:54.976396Z",
     "iopub.status.busy": "2023-06-10T00:30:54.975802Z",
     "iopub.status.idle": "2023-06-10T00:30:54.983680Z",
     "shell.execute_reply": "2023-06-10T00:30:54.982275Z",
     "shell.execute_reply.started": "2023-06-10T00:30:54.976356Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T00:30:54.986172Z",
     "iopub.status.busy": "2023-06-10T00:30:54.985616Z",
     "iopub.status.idle": "2023-06-10T00:30:57.913049Z",
     "shell.execute_reply": "2023-06-10T00:30:57.911348Z",
     "shell.execute_reply.started": "2023-06-10T00:30:54.986140Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 13:33:50.898008: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-27 13:33:51.012291: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-27 13:33:51.532974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Input\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T00:31:23.591038Z",
     "iopub.status.busy": "2023-06-10T00:31:23.590625Z",
     "iopub.status.idle": "2023-06-10T00:31:23.597632Z",
     "shell.execute_reply": "2023-06-10T00:31:23.596177Z",
     "shell.execute_reply.started": "2023-06-10T00:31:23.591003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T00:31:27.099150Z",
     "iopub.status.busy": "2023-06-10T00:31:27.098788Z",
     "iopub.status.idle": "2023-06-10T00:31:27.658404Z",
     "shell.execute_reply": "2023-06-10T00:31:27.657283Z",
     "shell.execute_reply.started": "2023-06-10T00:31:27.099118Z"
    }
   },
   "outputs": [],
   "source": [
    "benign_path = \"Dataset_BUSI_with_GT/benign\"\n",
    "normal_path = \"Dataset_BUSI_with_GT/normal\"\n",
    "malignant_path = \"Dataset_BUSI_with_GT/malignant\"\n",
    "benign_dataset = []\n",
    "normal_dataset = []\n",
    "malignant_dataset = []\n",
    "\n",
    "classes_path = [benign_path, normal_path, malignant_path]\n",
    "classes_dataset = [benign_dataset, normal_dataset, malignant_dataset]\n",
    "\n",
    "for index, _class in enumerate(classes_path):\n",
    "    for filename in os.listdir(_class):\n",
    "        image_path = os.path.join(_class, filename)\n",
    "        \n",
    "        if os.path.isfile(image_path) and filename.lower().endswith(').png'):\n",
    "            mask_path = filename.replace('.png', '_mask.png')\n",
    "            filename = os.path.join(_class, filename) \n",
    "            mask_path = os.path.join(_class, mask_path) \n",
    "            classes_dataset[index].append((filename ,mask_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T00:31:31.737145Z",
     "iopub.status.busy": "2023-06-10T00:31:31.735952Z",
     "iopub.status.idle": "2023-06-10T00:31:31.746741Z",
     "shell.execute_reply": "2023-06-10T00:31:31.745399Z",
     "shell.execute_reply.started": "2023-06-10T00:31:31.737106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the benign class is 437\n",
      "The length of the normal class is 133\n",
      "The length of the malignant class is 210\n",
      "\u001b[33m----------------------------------------\n",
      "\u001b[30mThe total length of the dataset is 780\n"
     ]
    }
   ],
   "source": [
    "color_yellow = '\\033[33m' \n",
    "color_black = '\\033[30m'\n",
    "\n",
    "length = 0\n",
    "for _class in classes_dataset:\n",
    "    print('The length of the {} class is {}'.format(_class[0][0].split('/')[-2], len(_class)))\n",
    "    length += len(_class)\n",
    "print(color_yellow + 40*'-')\n",
    "print(color_black + 'The total length of the dataset is ' + str(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dataset = [path for sublist in classes_dataset for path in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Dataset_BUSI_with_GT/benign/benign (147).png',\n",
       " 'Dataset_BUSI_with_GT/benign/benign (147)_mask.png')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(classes_dataset[1][0])\n",
    "mask = cv2.imread(classes_dataset[1][1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_image = cv2.addWeighted(image,0.6,mask,0.1,0)\n",
    "\n",
    "cv2.imwrite('combined.png', added_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('combined.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T00:48:23.906335Z",
     "iopub.status.busy": "2023-06-10T00:48:23.905938Z",
     "iopub.status.idle": "2023-06-10T00:48:23.916767Z",
     "shell.execute_reply": "2023-06-10T00:48:23.915836Z",
     "shell.execute_reply.started": "2023-06-10T00:48:23.906302Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttentionEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input image tensor of shape (batch_size, input_size)\n",
    "        hidden = torch.tanh(self.linear(x))\n",
    "        attention_weights = F.softmax(self.attention(hidden), dim=1)\n",
    "        context_vector = torch.sum(hidden * attention_weights, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, context_vector):\n",
    "        # context_vector: input context vector tensor of shape (batch_size, hidden_size)\n",
    "        output = self.linear(context_vector)\n",
    "        return output\n",
    "\n",
    "\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        self.encoder = AttentionEncoder(input_size, hidden_size)\n",
    "        self.decoder = AttentionDecoder(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input image tensor of shape (batch_size, input_size)\n",
    "        context_vector = self.encoder(x)\n",
    "        output = self.decoder(context_vector)\n",
    "        return output\n",
    "\n",
    "class ImageEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ImageEncoderDecoder, self).__init__()\n",
    "        self.encoder = AttentionEncoder(input_size, hidden_size)\n",
    "        self.decoder = AttentionDecoder(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context_vector = self.encoder(x)\n",
    "        output = self.decoder(context_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T00:48:25.365816Z",
     "iopub.status.busy": "2023-06-10T00:48:25.364862Z",
     "iopub.status.idle": "2023-06-10T00:48:25.447997Z",
     "shell.execute_reply": "2023-06-10T00:48:25.446671Z",
     "shell.execute_reply.started": "2023-06-10T00:48:25.365751Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have prepared your dataset and defined the model as 'model'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m ImageEncoderDecoder(\u001b[43minput_size\u001b[49m, hidden_size, num_classes)\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have prepared your dataset and defined the model as 'model'\n",
    "num_classes = 10\n",
    "\n",
    "model = ImageEncoderDecoder(input_size, hidden_size, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    accuracy = correct / len(valid_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the best model checkpoint\n",
    "torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# Load the best model checkpoint for testing\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
